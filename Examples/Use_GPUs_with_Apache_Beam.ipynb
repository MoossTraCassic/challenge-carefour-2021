{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "collectible-jonathan",
   "metadata": {},
   "source": [
    "##### Copyright 2021 Google Inc.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\").\n",
    "<!--\n",
    "    Licensed to the Apache Software Foundation (ASF) under one\n",
    "    or more contributor license agreements.  See the NOTICE file\n",
    "    distributed with this work for additional information\n",
    "    regarding copyright ownership.  The ASF licenses this file\n",
    "    to you under the Apache License, Version 2.0 (the\n",
    "    \"License\"); you may not use this file except in compliance\n",
    "    with the License.  You may obtain a copy of the License at\n",
    "\n",
    "      http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "    Unless required by applicable law or agreed to in writing,\n",
    "    software distributed under the License is distributed on an\n",
    "    \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n",
    "    KIND, either express or implied.  See the License for the\n",
    "    specific language governing permissions and limitations\n",
    "    under the License.\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "virtual-dealer",
   "metadata": {},
   "source": [
    "# Use GPUs with Apache Beam\n",
    "\n",
    "This notebook uses the [Monte Carlo method](https://en.wikipedia.org/wiki/Monte_Carlo_method) to calculate pi (3.14159...) and demonstrate the performance difference at the same scale of sample size between different runtimes:\n",
    "\n",
    "- Native Python code\n",
    "- Jitted machine code\n",
    "- On GPU (CUDA)\n",
    "- Beam pipeline locally on GPU\n",
    "\n",
    "**Note**: this notebook does not work if your notebook instance does not have a GPU or the drivers are not installed. If you haven't done so, create a [new Dataflow Notebooks](https://pantheon.corp.google.com/dataflow/notebooks/list/instances) instance `With 1 NVIDIA Tesla T4` and check the option to `Install NVIDIA GPU driver automatically for me`.\n",
    "\n",
    "More details can be found on [Dataflow support for GPUs](https://cloud.google.com/dataflow/docs/concepts/gpu-support) if you want to productionize Beam pipelines on Dataflow with GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "external-control",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "\n",
    "We will use [numba](https://numba.pydata.org/) to compile code in this notebook for different runtimes.\n",
    "\n",
    ">Numba is an open source JIT compiler that translates a subset of Python and NumPy code into fast machine code.\n",
    "\n",
    "It also supports GPU acceleration.\n",
    "\n",
    "**Note**: you might need to restart the kernel if this is the first time you've installed the package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "official-brief",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hispanic-checklist",
   "metadata": {},
   "source": [
    "Let's check if the CUDA libraries are available (*if not, your notebook instance probably wasn't created with a GPU*):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "invalid-northwest",
   "metadata": {},
   "outputs": [],
   "source": [
    "!find /usr/local/cuda-* -iname 'libdevice'\n",
    "!find /usr/local/cuda-* -iname 'libnvvm.so'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amino-documentary",
   "metadata": {},
   "source": [
    "Let's disable non-error logs for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tribal-husband",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger().setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civil-ministry",
   "metadata": {},
   "source": [
    "## Native Python & jitted machine code\n",
    "\n",
    "Below we have defined two functions with exactly the same code:\n",
    "\n",
    "- `python_cpu_monte_carlo_pi` is a plain native Python function that runs on the CPU.\n",
    "- `machine_code_cpu_monte_carlo_pi` has an annotation `@jit(nopython=True)` that compiles the Python code into machine code that runs on the CPU too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premium-arena",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "from numba import jit\n",
    "\n",
    "\n",
    "def python_cpu_monte_carlo_pi(sample_size):\n",
    "    acc = 0\n",
    "    for i in range(sample_size):\n",
    "        x = random.random()\n",
    "        y = random.random()\n",
    "        if (x * x + y * y) < 1.0:\n",
    "            acc += 1\n",
    "    return 4.0 * acc / sample_size\n",
    "\n",
    "\n",
    "@jit(nopython=True)\n",
    "def machine_code_cpu_monte_carlo_pi(sample_size):\n",
    "    acc = 0\n",
    "    for i in range(sample_size):\n",
    "        x = random.random()\n",
    "        y = random.random()\n",
    "        if (x * x + y * y) < 1.0:\n",
    "            acc += 1\n",
    "    return 4.0 * acc / sample_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latest-format",
   "metadata": {},
   "source": [
    "Let's choose a sample size (100,000,000) as a constant computation complexity between both runs.\n",
    "\n",
    "The most expensive yet parallelizable part of the computation is generating the random numbers. Neither function optimizes that part though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equal-glucose",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 100_000_000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selective-warrant",
   "metadata": {},
   "source": [
    "### Performance of native Python code\n",
    "\n",
    "It should take ~40 seconds for native Python code to calculate pi with a sample size of 100,000,000.\n",
    "\n",
    "**Note**: The performance might vary from run to run. It might also vary between different notebook instances if they are configured differently. The same applies to other runtimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collected-probe",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = timer()\n",
    "pi = python_cpu_monte_carlo_pi(sample_size)\n",
    "dt = timer() - start\n",
    "print(f'Monte Carlo pi calculated as {pi} in {dt} s.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "later-collective",
   "metadata": {},
   "source": [
    "### Performance of jitted machine code\n",
    "\n",
    "It should only take a little over 1 second for jitted machine code to calculate pi with a sample size of 100,000,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decimal-sight",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = timer()\n",
    "pi = machine_code_cpu_monte_carlo_pi(sample_size)\n",
    "dt = timer() - start\n",
    "print(f'Monte Carlo pi calculated as {pi} in {dt} s.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liable-dimension",
   "metadata": {},
   "source": [
    "## On GPU (CUDA)\n",
    "\n",
    "In the below example, we have rearranged the native Python function into two parts:\n",
    "\n",
    "- The first part, `gpu_monte_carlo_pi_sampler`, which generates random points and aggregate counts for a subset of the target sample size, is executed on the GPU.\n",
    "- The second part, `calculate_pi`, which aggregates value from all sub sample sets and calculates pi, is compiled into machine code and executed on the GPU.\n",
    "\n",
    "**Note**:`njit` is similar to `jit` but for `numpy`.\n",
    "\n",
    "The example code uses only 1 grid, 24 blocks and 64 threads per block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "congressional-active",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda, njit\n",
    "from numba.cuda.random import create_xoroshiro128p_states, xoroshiro128p_uniform_float32\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "@cuda.jit\n",
    "def gpu_monte_carlo_pi_sampler(rng_states, sub_sample_size, acc):\n",
    "    \"\"\"Uses GPU to sample random values and accumulates the sub count\n",
    "    of values within the circle.\n",
    "    \"\"\"\n",
    "    pos = cuda.grid(1)\n",
    "    if pos < acc.size:\n",
    "        sub_acc = 0\n",
    "        for i in range(sub_sample_size):\n",
    "            x = xoroshiro128p_uniform_float32(rng_states, pos)\n",
    "            y = xoroshiro128p_uniform_float32(rng_states, pos)\n",
    "            if (x * x + y * y) < 1.0:\n",
    "                sub_acc += 1\n",
    "        acc[pos] = sub_acc\n",
    "\n",
    "\n",
    "@njit(fastmath=True)\n",
    "def calculate_pi(acc, sample_size):\n",
    "    \"\"\"Uses machine code on CPU to aggregate and calculate pi since there\n",
    "    is less parallelism here.\n",
    "    \"\"\"\n",
    "    return 4 * np.sum(acc) / sample_size\n",
    "\n",
    "threadsperblock = 64\n",
    "blocks = 24\n",
    "# An 1-d array to hold sub count of acc.\n",
    "acc = np.zeros(threadsperblock * blocks, dtype=np.float32)\n",
    "rng_states = create_xoroshiro128p_states(threadsperblock * blocks, seed=1)\n",
    "# Each thread should only generate sub_sample_size of random points.\n",
    "sub_sample_size = sample_size // acc.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disturbed-gamma",
   "metadata": {},
   "source": [
    "### Performance of on GPU (CUDA)\n",
    "\n",
    "It should take ~0.3 second to calculate pi on a GPU using above configuration with a sample size of 100,000,000.\n",
    "\n",
    "It's **130 times faster** than native Python on CPU and **3 times** faster than machine code on CPU. And we haven't even pushed the single GPU to its limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spectacular-search",
   "metadata": {},
   "outputs": [],
   "source": [
    "start=timer()\n",
    "gpu_monte_carlo_pi_sampler[blocks, threadsperblock](rng_states, sub_sample_size, acc)\n",
    "pi = calculate_pi(acc, sample_size)\n",
    "dt = timer() - start\n",
    "print(f'Monte Carlo pi calculated as {pi} in {dt} s.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cubic-triangle",
   "metadata": {},
   "source": [
    "## Running a Beam pipeline locally on GPU\n",
    "\n",
    "It might not be obvious why you need to wrap your code that can already run on a GPU\n",
    "into an Apache Beam pipeline.\n",
    "\n",
    "The advantage of Beam with GPU is that you can later run the pipeline on Dataflow or any other runner/clusters\n",
    "so that you can distribute/scale the work to as many GPUs as you want.\n",
    "\n",
    "Note: you might need to configure worker machines so that they have GPU devices and drivers available.\n",
    "For example, [using GPUs with Dataflow](https://cloud.google.com/dataflow/docs/guides/using-gpus).\n",
    "You also need to constrain your GPU usages to work within the hardware limit. See\n",
    "[GPUs and worker parallelism](https://cloud.google.com/dataflow/docs/concepts/gpu-support#gpus_and_worker_parallelism).\n",
    "\n",
    "In the below example, we build a Beam pipeline with code similar to the plain [on-GPU example](#On-GPU-(CUDA)) and run the pipeline locally on this notebook instance.\n",
    "\n",
    "First, we create a pipeline instance with options that utilize all CPU cores to schedule work when running the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developing-ordering",
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing as t\n",
    "\n",
    "import apache_beam as beam\n",
    "from apache_beam.options import pipeline_options\n",
    "from apache_beam.runners.interactive.interactive_runner import InteractiveRunner\n",
    "from apache_beam.runners.interactive import interactive_beam as ib\n",
    "\n",
    "\n",
    "options = pipeline_options.PipelineOptions(\n",
    "    direct_num_workers=0,  # default threads/subprocess to the number of cores on this machine\n",
    "    direct_running_mode='multi_threading')\n",
    "p = beam.Pipeline(InteractiveRunner(), options=options)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "usual-swiss",
   "metadata": {},
   "source": [
    "Then, we define a `DoFn` as a `Sampler` that uses the GPU to process elements.\n",
    "\n",
    "Each element is a tuple of 2 `int` values:\n",
    "\n",
    "- first value: the seed of a random number generator\n",
    "- second value: the size of sample values to be generated\n",
    "\n",
    "The `DoFn` itself runs as native Python code on the CPU while the inner logic of `gpu_monte_carlo_pi_sampler` runs on GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strategic-poverty",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampler(beam.DoFn):\n",
    "    def process(self, element: t.Tuple[int, int]):\n",
    "        rng_seed = element[0]\n",
    "        sample_size_per_sampler = element[1]\n",
    "\n",
    "        threadsperblock_per_sampler = 64\n",
    "        blocks_per_sampler = 24\n",
    "        acc_per_sampler = np.zeros(\n",
    "            threadsperblock_per_sampler * blocks_per_sampler, dtype=np.float32)\n",
    "        rng_states_per_sampler = create_xoroshiro128p_states(\n",
    "            threadsperblock_per_sampler * blocks_per_sampler, seed=rng_seed)\n",
    "        sub_sample_size_per_thread = sample_size_per_sampler // acc_per_sampler.shape[0]\n",
    "        gpu_monte_carlo_pi_sampler[blocks_per_sampler, threadsperblock_per_sampler](\n",
    "            rng_states_per_sampler, sub_sample_size_per_thread, acc_per_sampler)\n",
    "        yield acc_per_sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dental-republic",
   "metadata": {},
   "source": [
    "We can divide the work to 100 samplers. In a distributed environment, each sampler can run on a different machine with its own GPU(s).\n",
    "\n",
    "Here for simplicity, we run the pipeline locally and all samplers would share the same GPU on this notebook instance.\n",
    "\n",
    "We start the pipeline by creating a PCollection of 100 tuples of random number seeds (from 0 to 99) and sample size per sampler (1,000,000).\n",
    "\n",
    "Then we let the `Sampler DoFn` take these tuples as inputs to generate sample values.\n",
    "\n",
    "Each sampler has threadsperblock_per_sampler * blocks_per_sampler = **1536 threads** running in parallel\n",
    "**on the GPU**. \n",
    "\n",
    "And we have **100 samplers** running concurrently scheduled by Beam **on all CPU cores** on the machine running this notebook.\n",
    "\n",
    "In the data visualization, you can see the aggregated counts of \"random points within the circle\" for each GPU thread by each sampler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saving-watershed",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler_count = 100\n",
    "sample_size_per_sampler = sample_size // sampler_count\n",
    "\n",
    "\n",
    "samplers_per_gpu_thread = (p \n",
    "                           | beam.Create([(i, sample_size_per_sampler) for i in range(sampler_count)])\n",
    "                           | beam.ParDo(Sampler()))\n",
    "\n",
    "# The output might be too big to be saved in an ipynb file, you can clear it before saving your work.\n",
    "ib.show(samplers_per_gpu_thread)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organizational-objective",
   "metadata": {},
   "source": [
    "Everything below runs as native Python code on the CPU.\n",
    "\n",
    "To calculate pi, we need to aggregate all values from all GPU threads for each sampler.\n",
    "\n",
    "**Note**: we use numpy to sum the values (`np.float32`) and then convert them back to the native Python type (`int`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "middle-horror",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_per_sampler = samplers_per_gpu_thread | beam.Map(lambda x: np.sum(x).astype(int).item()).with_output_types(int)\n",
    "\n",
    "# Sum up per-gpu-thread count of values falling into the circle of Monte Carlo pi calculation for each sampler.\n",
    "ib.show(acc_per_sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "successful-subdivision",
   "metadata": {},
   "source": [
    "Then we combine all values from all samplers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "relevant-reunion",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_acc = acc_per_sampler | beam.CombineGlobally(sum)\n",
    "\n",
    "# Sum up the count of values falling into the circle of Monte Carlo pi calculation from all samplers.\n",
    "ib.show(sum_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retained-heather",
   "metadata": {},
   "source": [
    "Once we have all values aggregated, we can use the formula to calculate and print pi.\n",
    "\n",
    "The visualization shows the pipeline graph. Let's make sure it's correctly constructed and doesn't have corrupted states from notebook executions.\n",
    "\n",
    "**Note**: if the graph is mixed with transforms that are applied by out-of-order execution and re-execution of notebooks, please re-execute the code\n",
    "from where the pipeline is created or restart the kernel and re-execute the whole notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunset-immunology",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_pi = sum_acc | beam.Map(lambda x: print(4 * x / sample_size))\n",
    "            \n",
    "ib.show_graph(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "activated-ethiopia",
   "metadata": {},
   "source": [
    "### Performance of Beam pipeline locally on GPU\n",
    "\n",
    "It should only take 2.6 seconds to calculate pi using Beam on GPU with a sample size of 100,000,000.\n",
    "\n",
    "Though it's not as fast as pure GPU + machine code executed on a single machine, it provides the scalability to run the code\n",
    "on distributed systems, and the performance is almost on par with pure machine code on a single machine. You can also improve the\n",
    "performance further by compiling those transforms written in native Python code into machine code with jit/njit.\n",
    "\n",
    "The example has demonstrated how to write a Beam pipeline using a GPU, the performance increment when using a GPU compared to native Python code on CPU, and the small overhead of a Beam pipeline on a local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liked-norway",
   "metadata": {},
   "outputs": [],
   "source": [
    "start=timer()\n",
    "print('Monte Carlo pi calculated as: ')\n",
    "p.run()\n",
    "dt = timer() - start\n",
    "print(f'in {dt} s.')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
